{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_how_to_add_a_domain_specific_vocabulary_new_tokens_to_a_subword_tokenizer_already_trained_like_BERT_WordPiece.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "GaH0DSElUHyP",
        "luZPFwswULij",
        "DI_Q7C-kUT5k",
        "lC2Y9cHCUpzk"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ej5ImPsRvTN"
      },
      "source": [
        "# NLP | How to add a domain-specific vocabulary (new tokens) to a subword tokenizer already trained like BERT WordPiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9pW-pZ0aGUp"
      },
      "source": [
        "* **Author**: [Pierre GUILLOU](https://www.linkedin.com/in/pierreguillou/)\n",
        "* **Date**: April 05, 2021\n",
        "* **Blog post**: [NLP & domain specific | How to add a specialized vocabulary (new tokens) to a subword tokenizer already trained like BERT WordPiece](https://medium.com/@pierre_guillou/nlp-how-to-add-a-domain-specific-vocabulary-new-tokens-to-a-subword-tokenizer-already-trained-33ab15613a41)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNxQvuVqEzYQ"
      },
      "source": [
        "**Summary**: In some cases, it may be crucial to enrich the vocabulary of an already trained natural language model with vocabulary from a specialized domain (medicine, law, etc.) in order to perform new tasks (classification, NER, summary, translation, etc.). While the Hugging Face library allows you to easily add new tokens to the vocabulary of an existing tokenizer like BERT WordPiece, those tokens must be whole words, not subwords. This article explains why and how to obtain these new tokens from a specialized corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaH0DSElUHyP"
      },
      "source": [
        "## Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Voe1oG_wHTj",
        "outputId": "5dd4807c-4969-434a-965d-3660975283b5"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Wed_Jul_22_19:09:09_PDT_2020\n",
            "Cuda compilation tools, release 11.0, V11.0.221\n",
            "Build cuda_11.0_bu.TC445_37.28845127_0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Gi4ZoDXOR1mm",
        "outputId": "e89eb876-c0ae-45ef-b429-2a806c5ae12d"
      },
      "source": [
        "# Install last Hugging Face libraries (datasets & transformers)\n",
        "!pip install datasets git+https://github.com/huggingface/transformers/\n",
        "# install spaCY\n",
        "!pip install -U pip setuptools wheel\n",
        "!pip install -U spacy[cuda110]\n",
        "!python -m spacy download en_core_web_sm\n",
        "# install scikit-learn\n",
        "!pip install -U scikit-learn\n",
        "# install matplotlib\n",
        "!pip install matplotlib\n",
        "# install wikipedia\n",
        "!pip install wikipedia"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers/\n",
            "  Cloning https://github.com/huggingface/transformers/ to /tmp/pip-req-build-kvgmr32c\n",
            "  Running command git clone -q https://github.com/huggingface/transformers/ /tmp/pip-req-build-kvgmr32c\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting datasets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/1a/b9f9b3bfef624686ae81c070f0a6bb635047b17cdb3698c7ad01281e6f9a/datasets-1.6.2-py3-none-any.whl (221kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 4.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 38.0MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 34.1MB/s \n",
            "\u001b[?25hCollecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (4.0.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Collecting fsspec\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/52/816d1a3a599176057bf29dfacb1f8fadb61d35fbd96cb1bab4aaa7df83c0/fsspec-2021.5.0-py3-none-any.whl (111kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 39.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=1.0.0<4.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.11.1)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 37.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.7.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.7.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.7.0.dev0) (8.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0.dev0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0.dev0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.7.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.7.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.7.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.7.0.dev0-cp37-none-any.whl size=2269400 sha256=8ee6f44e14eb2d33435e70be6eb64091a2f64681f52882921d095ad2cb53ed8a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-egrwpmns/wheels/61/69/33/974fccec4d0ab5feee9fe83bd93e680d269a805be9ede5ec60\n",
            "Successfully built transformers\n",
            "Installing collected packages: fsspec, huggingface-hub, xxhash, datasets, sacremoses, tokenizers, transformers\n",
            "Successfully installed datasets-1.6.2 fsspec-2021.5.0 huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.7.0.dev0 xxhash-2.0.2\n",
            "Collecting pip\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/6f/43037c7bcc8bd8ba7c9074256b1a11596daa15555808ec748048c1507f08/pip-21.1.1-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 4.1MB/s \n",
            "\u001b[?25hCollecting setuptools\n",
            "  Using cached https://files.pythonhosted.org/packages/d0/15/5041473f5d142ee93bf1593deb8f932e27a078f6f04e2020cf44044f72c5/setuptools-56.2.0-py3-none-any.whl\n",
            "Requirement already up-to-date: wheel in /usr/local/lib/python3.7/dist-packages (0.36.2)\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pip, setuptools\n",
            "  Found existing installation: pip 19.3.1\n",
            "    Uninstalling pip-19.3.1:\n",
            "      Successfully uninstalled pip-19.3.1\n",
            "  Found existing installation: setuptools 56.1.0\n",
            "    Uninstalling setuptools-56.1.0:\n",
            "      Successfully uninstalled setuptools-56.1.0\n",
            "Successfully installed pip-21.1.1 setuptools-56.2.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy[cuda110] in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "\u001b[33mWARNING: spacy 2.2.4 does not provide the extra 'cuda110'\u001b[0m\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda110]) (0.8.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda110]) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy[cuda110]) (56.2.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda110]) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda110]) (2.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda110]) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda110]) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda110]) (2.23.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda110]) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda110]) (3.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda110]) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda110]) (4.41.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda110]) (1.1.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy[cuda110]) (4.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy[cuda110]) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy[cuda110]) (3.7.4.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda110]) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda110]) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda110]) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda110]) (2.10)\n",
            "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (56.2.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.22.2.post1)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 36 kB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Installing collected packages: threadpoolctl, scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-0.24.2 threadpoolctl-2.1.0\n",
            "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.19.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
            "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11686 sha256=2803ebb55dc0f0e68a4d3e801f442699d85f54b71dce1e401bde289bfb251ce2\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/93/6d/5b2c68b8a64c7a7a04947b4ed6d89fb557dcc6bc27d1d7f3ba\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n",
            "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luZPFwswULij"
      },
      "source": [
        "## Download a BERT model and its WordPiece tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnV5TNmUR2QL",
        "outputId": "f45c9d58-9524-47af-b1dd-adaa1b219d3e"
      },
      "source": [
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "model_name = \"bert-base-cased\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_name)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DI_Q7C-kUT5k"
      },
      "source": [
        "## Tokenize a phrase about COVID"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9u3-n92Sska"
      },
      "source": [
        "text = \"COVID-19 affects different people in different ways. Most infected people will develop mild to moderate illness and recover without hospitalization.\""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j97tA5WeS2yV",
        "outputId": "5087f904-aace-4c42-bbc5-5124e932022d"
      },
      "source": [
        "# tokenization of the text\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(tokens)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['CO', '##VI', '##D', '-', '19', 'affects', 'different', 'people', 'in', 'different', 'ways', '.', 'Most', 'infected', 'people', 'will', 'develop', 'mild', 'to', 'moderate', 'illness', 'and', 'recover', 'without', 'hospital', '##ization', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LgH1SvInzW11",
        "outputId": "f0be8d33-cfd3-411a-ecd7-f6bfe7a74b10"
      },
      "source": [
        "# back to text\n",
        "tokenizer.decode(tokenizer.encode(text), skip_special_tokens=True)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'COVID - 19 affects different people in different ways. Most infected people will develop mild to moderate illness and recover without hospitalization.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMa--sCnJp4Z",
        "outputId": "3d6c5753-5bb0-4bae-f3be-eee6bb150896"
      },
      "source": [
        "print(tokenizer.tokenize('COVID'))\n",
        "print(tokenizer.tokenize('hospitalization'))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['CO', '##VI', '##D']\n",
            "['hospital', '##ization']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXwgWX9zpRHX"
      },
      "source": [
        "**We can notice that the BERT WordPiece tokenizer (from the bert-base-cased model) tokenize the words COVID and hospitalization with subwords because they do not exist as words in the tokenizer vocabulary.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrETrcNdFt7i",
        "outputId": "17edffdb-021b-4ce9-bca6-87b775f77601"
      },
      "source": [
        "# Verify that the words COVID and hospitalization DO NOT belong to the tokenizer vocabulary\n",
        "vocab = [tok for tok,index in tokenizer.get_vocab().items()]\n",
        "\"COVID\" in vocab, \"hospitalization\" in vocab"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(False, False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lC2Y9cHCUpzk"
      },
      "source": [
        "## [ First test ] Add 2 new tokens (whole words) into the tokenizer vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR68tWtPTTen"
      },
      "source": [
        "new_tokens = ['COVID', 'hospitalization']"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3vXXLqPSWBg",
        "outputId": "a44353b7-ef90-40ac-f5de-b417272cb8f7"
      },
      "source": [
        "print(\"[ BEFORE ] tokenizer vocab size:\", len(tokenizer)) \n",
        "added_tokens = tokenizer.add_tokens(new_tokens)\n",
        "\n",
        "print(\"[ AFTER ] tokenizer vocab size:\", len(tokenizer)) \n",
        "print()\n",
        "print('added_tokens:',added_tokens)\n",
        "print()\n",
        "\n",
        "# resize the embeddings matrix of the model \n",
        "model.resize_token_embeddings(len(tokenizer)) "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ BEFORE ] tokenizer vocab size: 28996\n",
            "[ AFTER ] tokenizer vocab size: 28998\n",
            "\n",
            "added_tokens: 2\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(28998, 768)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oO8ruf7-LFQR",
        "outputId": "d84475dd-fc05-43b4-b707-2692e24dd0e1"
      },
      "source": [
        "# Verify that the words COVID and hospitalization DO belong to the tokenizer vocabulary\n",
        "vocab = [tok for tok,index in tokenizer.get_vocab().items()]\n",
        "\"COVID\" in vocab, \"hospitalization\" in vocab"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOz_GyDyIUTG"
      },
      "source": [
        "Let's call tokenizer_exBERT our tokenizer with the 2 new tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzpHF5Fvzs5X"
      },
      "source": [
        "tokenizer_exBERT = tokenizer"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jh1C-86lTN-8",
        "outputId": "d356d3ee-9d15-4c6f-b5bb-5ea0c23b322b"
      },
      "source": [
        "# tokenization of the text\n",
        "tokens = tokenizer_exBERT.tokenize(text)\n",
        "print(tokens)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['COVID', '-', '19', 'affects', 'different', 'people', 'in', 'different', 'ways', '.', 'Most', 'infected', 'people', 'will', 'develop', 'mild', 'to', 'moderate', 'illness', 'and', 'recover', 'without', 'hospitalization', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "H9iJexuPTzAU",
        "outputId": "a0b1bd08-5cb7-4bc5-b89a-63bd09f4650e"
      },
      "source": [
        "# back to text\n",
        "tokenizer_exBERT.decode(tokenizer_exBERT.encode(text), skip_special_tokens=True)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'COVID - 19 affects different people in different ways. Most infected people will develop mild to moderate illness and recover without hospitalization.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcBX4c8hIl29"
      },
      "source": [
        "**The tokenizer with the 2 new tokens succeeded in tokenizing the words COVID and hospitalization without subwords as they belong now to the vocabulary tokenizer.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzWCYzijJA4H",
        "outputId": "4aa13a9d-4a4a-4d67-b4f6-5141490e87a5"
      },
      "source": [
        "# tokenization of the words COVID and hospitalization\n",
        "print(tokenizer_exBERT.tokenize('COVID'))\n",
        "print(tokenizer_exBERT.tokenize('hospitalization'))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['COVID']\n",
            "['hospitalization']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaKONVnBVHav"
      },
      "source": [
        "## [ Second test ] Add more new tokens (subwords and words) into the tokenizer vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k58Ya5neFvxj"
      },
      "source": [
        "What if we want to detect the whole vocabulary of a specialized corpus (and not only 2 words) in order to add it to an existing corpus? \n",
        "\n",
        "Let's use a WordpIece tokenizer for this! (Why a WordPiece tokenizer? This is our first guess: since the BERT tokenizer is a WordPiece tokenizer, let's use a tokenizer of the same type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcgpRtetz-ZE"
      },
      "source": [
        "### 1) Import pages about COVID from English Wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XKt2He0v0kH",
        "outputId": "dab74b62-ce7a-4b83-b9ed-ea755d939ff7"
      },
      "source": [
        "import wikipedia\n",
        "\n",
        "# let's choose 2 Wikipedia pages for our demonstration (we could have choosen an infinity)\n",
        "pages = [\"COVID-19\",\"COVID-19 pandemic\"]\n",
        "\n",
        "documents = list()\n",
        "for p in pages:\n",
        "  page = wikipedia.page(p)\n",
        "  documents.append(page.content)\n",
        "  print(page.title,page.url)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "COVID-19 https://en.wikipedia.org/wiki/COVID-19\n",
            "COVID-19 pandemic https://en.wikipedia.org/wiki/COVID-19_pandemic\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AP5hQgIV0Fz6"
      },
      "source": [
        "### 2) Train a WordPiece tokenizer on the imported Wikipedia pages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTYi2eC30Mw7"
      },
      "source": [
        "Source: [All together: a BERT tokenizer from scratch](https://huggingface.co/docs/tokenizers/python/latest/pipeline.html#all-together-a-bert-tokenizer-from-scratch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIcwCvJmvBtp"
      },
      "source": [
        "# tokenzer WordPiece\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordPiece\n",
        "\n",
        "bert_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
        "\n",
        "# normalizer\n",
        "from tokenizers import normalizers\n",
        "from tokenizers.normalizers import Lowercase, NFD, StripAccents\n",
        "\n",
        "bert_tokenizer.normalizer = normalizers.Sequence([NFD()])\n",
        "\n",
        "# pre-tokenizer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "bert_tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "# template\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "bert_tokenizer.post_processor = TemplateProcessing(\n",
        "    single=\"[CLS] $A [SEP]\",\n",
        "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
        "    special_tokens=[\n",
        "        (\"[CLS]\", 1),\n",
        "        (\"[SEP]\", 2),\n",
        "    ],\n",
        ")\n",
        "\n",
        "# instantiate a trainer\n",
        "from tokenizers.trainers import WordPieceTrainer\n",
        "trainer = WordPieceTrainer(\n",
        "    vocab_size=30522, \n",
        "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
        "    )\n",
        "\n",
        "# Train \n",
        "files = documents\n",
        "bert_tokenizer.train_from_iterator(files, trainer)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsRvZ32q0etr"
      },
      "source": [
        "### 3) Get the vocabulary that is not in the original BERT tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yxColA0BQ5e"
      },
      "source": [
        "This step is not necessary, as the `tokenizer.add_tokens()` method will add new tokens only if they do not belong to the existing tokenizer vocabulary. However, it helps us to see what these new tokens are."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNmp_5H7VVIf"
      },
      "source": [
        "old_vocab = [k for k,v in tokenizer.get_vocab().items()]\n",
        "new_vocab = [k for k,v in bert_tokenizer.get_vocab().items()]\n",
        "idx_old_vocab_list = list()\n",
        "same_tokens_list = list()\n",
        "different_tokens_list = list()\n",
        "\n",
        "for idx_new,w in enumerate(new_vocab): \n",
        "  try:\n",
        "    idx_old = old_vocab.index(w)\n",
        "  except:\n",
        "    idx_old = -1\n",
        "  if idx_old>=0:\n",
        "      idx_old_vocab_list.append(idx_old)\n",
        "      same_tokens_list.append((w,idx_new))\n",
        "  else:\n",
        "      different_tokens_list.append((w,idx_new))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7rIlrLcxK1u",
        "outputId": "14cca16a-38c3-4413-9c79-79c8ce739345"
      },
      "source": [
        "len(same_tokens_list),len(different_tokens_list),len(same_tokens_list)+len(different_tokens_list)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4747, 3666, 8413)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dtxGtdwP9JW",
        "outputId": "c2896021-c070-4d95-8293-d1e4a551062a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        ""
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Protocol']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2VXO6LXSp4f"
      },
      "source": [
        "**We found 3651 tokens (subwords or words) that are not in the vocabulary of the original tokenizer.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CL7I-xL4yfJf",
        "outputId": "3e4290f2-fb76-4dc0-94c3-e619efba8974"
      },
      "source": [
        "# get list of new tokens\n",
        "new_tokens = [k for k,v in different_tokens_list]\n",
        "len(new_tokens), new_tokens[:10]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3666,\n",
              " ['Guilds',\n",
              "  'TN',\n",
              "  '##ogle',\n",
              "  'Available',\n",
              "  '##hion',\n",
              "  '##arating',\n",
              "  'val',\n",
              "  '##ominantly',\n",
              "  'cor',\n",
              "  '##ension'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehq09ep2mQKj"
      },
      "source": [
        "### 4) Add the new tokens (subwords and words) in the vocabulary of the original BERT tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1krFfLlsra1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebca75dd-8fb5-4225-9342-381de2b47e67"
      },
      "source": [
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "model_name = \"bert-base-cased\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_name)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-sMnbkHrjx9",
        "outputId": "42db07b8-c96a-4c1a-dfe6-45c82e956ec8"
      },
      "source": [
        "print(\"[ BEFORE ] tokenizer vocab size:\", len(tokenizer)) \n",
        "added_tokens = tokenizer.add_tokens(new_tokens)\n",
        "\n",
        "print(\"[ AFTER ] tokenizer vocab size:\", len(tokenizer)) \n",
        "print()\n",
        "print('added_tokens:',added_tokens)\n",
        "print()\n",
        "\n",
        "# resize the embeddings matrix of the model \n",
        "model.resize_token_embeddings(len(tokenizer)) "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ BEFORE ] tokenizer vocab size: 28996\n",
            "[ AFTER ] tokenizer vocab size: 32662\n",
            "\n",
            "added_tokens: 3666\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(32662, 768)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKN_fGADOL-Q",
        "outputId": "23735e36-2d9e-4a57-d840-4ff86029b727"
      },
      "source": [
        "# Verify if  the words COVID and hospitalization belong or not to the tokenizer vocabulary\n",
        "vocab = [tok for tok,index in tokenizer.get_vocab().items()]\n",
        "\"COVID\" in vocab, \"hospitalization\" in vocab"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(False, False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPLK_yTNWHlP"
      },
      "source": [
        "Let's call tokenizer_exBERT our tokenizer with the new tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njAuBZSnWHlR"
      },
      "source": [
        "tokenizer_exBERT = tokenizer"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrIwgDkGWHlS",
        "outputId": "fb384349-249e-410e-da7d-573e107b728a"
      },
      "source": [
        "# tokenization of the text\n",
        "tokens = tokenizer_exBERT.tokenize(text)\n",
        "print(tokens)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['COV', 'ID', '-', '19', 'affec', 't', '##s', 'dif', 'fe', 'rent', 'pe', 'o', 'ple', 'in', 'dif', 'fe', 'rent', 'ways', '.', 'Mo', 'st', 'in', 'fe', 'c', '##ted', 'pe', 'o', 'ple', 'will', 'd', 'ev', 'e', 'lop', 'mil', 'd', 'to', 'mod', 'e', 'ra', 'te', 'ill', '##n', 'ess', 'and', 'rec', 'over', 'without', 'ho', 'sp', 'i', 'tal', 'i', '##zation', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "Fzc1vszBWHlU",
        "outputId": "19c97914-b77a-4d0e-8293-52cdeee28369"
      },
      "source": [
        "# back to text\n",
        "tokenizer_exBERT.decode(tokenizer_exBERT.encode(text), skip_special_tokens=True)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'COV ID - 19 affec ts dif fe rent pe o ple in dif fe rent ways. Mo st in fe cted pe o ple will d ev e lop mil d to mod e ra te illn ess and rec over without ho sp i tal ization.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb7f6GZ8WHlV"
      },
      "source": [
        "**As the words COVID and hospitalization do not belong to the tokenizer vocabulary, they continue to be tokenized with subwords. That's right.**\n",
        "\n",
        "**However, only the word COVID is well tokenized: the word hospitalization is tokenized with subwords that do not start with ##. But except the first token, all other subword tokens should have started with ##!**\n",
        "\n",
        "**And we can see that many other words in the sentence are not well tokenized, too.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0N541hTWHlW",
        "outputId": "f95992d9-73a1-46f4-8c9c-b92c6d058858"
      },
      "source": [
        "# tokenization of the words COVID and hospitalization\n",
        "print(tokenizer_exBERT.tokenize('COVID'))\n",
        "print(tokenizer_exBERT.tokenize('hospitalization'))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['COVID']\n",
            "['hospitalization']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpvHAok0mU0r"
      },
      "source": [
        "### 5) Add only the new tokens that do not start with ## in the vocabulary of the original BERT tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YN6TITaiCBBt"
      },
      "source": [
        "We know that a subword is not just a token that starts with ##, but let's see what happens if we remove all those subwords from the list of new tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yL_L7DxYmgP9",
        "outputId": "e26468d5-0a5e-42b8-ad6a-782569cb0820"
      },
      "source": [
        "# get list of new tokens as whole words\n",
        "new_tokens = [tok for tok in new_tokens if tok.startswith(\"#\") == False]\n",
        "len(new_tokens), new_tokens[:10]"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2499,\n",
              " ['Guilds',\n",
              "  'TN',\n",
              "  'Available',\n",
              "  'val',\n",
              "  'cor',\n",
              "  'Recommend',\n",
              "  'aggravation',\n",
              "  'Thorough',\n",
              "  'Asians',\n",
              "  'Ursul'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fDb-siInHha",
        "outputId": "94b3e331-1fda-418b-c8ad-361cd5fb8806"
      },
      "source": [
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "model_name = \"bert-base-cased\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_name)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ko6CfvHRnHhc",
        "outputId": "f6460c1b-3daa-480e-c598-4c7b46cdf4dd"
      },
      "source": [
        "print(\"[ BEFORE ] tokenizer vocab size:\", len(tokenizer)) \n",
        "added_tokens = tokenizer.add_tokens(new_tokens)\n",
        "\n",
        "print(\"[ AFTER ] tokenizer vocab size:\", len(tokenizer)) \n",
        "print()\n",
        "print('added_tokens:',added_tokens)\n",
        "print()\n",
        "\n",
        "# resize the embeddings matrix of the model \n",
        "model.resize_token_embeddings(len(tokenizer)) "
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ BEFORE ] tokenizer vocab size: 28996\n",
            "[ AFTER ] tokenizer vocab size: 31495\n",
            "\n",
            "added_tokens: 2499\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(31495, 768)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d95HnOr3nHhe"
      },
      "source": [
        "Let's call tokenizer_exBERT our tokenizer with the new tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Nj1S1WwnHhf"
      },
      "source": [
        "tokenizer_exBERT = tokenizer"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J00Kg8L0nHhg",
        "outputId": "67256af6-5cc2-4b04-9dc2-7cf4b292bd08"
      },
      "source": [
        "# tokenization of the text\n",
        "tokens = tokenizer_exBERT.tokenize(text)\n",
        "print(tokens)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['COV', 'ID', '-', '19', 'affec', 't', '##s', 'dif', 'fe', 'rent', 'pe', 'o', 'ple', 'in', 'dif', 'fe', 'rent', 'ways', '.', 'Mo', 'st', 'in', 'fe', 'c', '##ted', 'pe', 'o', 'ple', 'will', 'd', 'ev', 'e', 'lop', 'mil', 'd', 'to', 'mod', 'e', 'ra', 'te', 'ill', '##n', 'ess', 'and', 'rec', 'over', 'without', 'ho', 'sp', 'i', 'tal', 'i', '##zation', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "2hPy_t1snHhh",
        "outputId": "a9f92e93-eca2-4961-f86b-8b76991200e4"
      },
      "source": [
        "# back to text\n",
        "tokenizer_exBERT.decode(tokenizer_exBERT.encode(text), skip_special_tokens=True)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'COV ID - 19 affec ts dif fe rent pe o ple in dif fe rent ways. Mo st in fe cted pe o ple will d ev e lop mil d to mod e ra te illn ess and rec over without ho sp i tal ization.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXUmp1COnHhi"
      },
      "source": [
        "**The tokenizer continues to fail!**\n",
        "\n",
        "**It means that we must improve the new tokens list by taking out as well the subwords that begin a word (ie, they don't start by ##).**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1Vn7QEPnHhj",
        "outputId": "77d91824-7b97-49b6-d3fa-18d999080b0a"
      },
      "source": [
        "# tokenization of the words COVID and hospitalization\n",
        "print(tokenizer_exBERT.tokenize('COVID'))\n",
        "print(tokenizer_exBERT.tokenize('hospitalization'))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['COV', 'ID']\n",
            "['ho', 'sp', 'i', 'tal', 'i', '##zation']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWzH7OfSDLUp"
      },
      "source": [
        "## [ Third test ] Add new tokens (only words, not subwords) into the tokenizer vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJO7bBUPyG2D"
      },
      "source": [
        "Let's add only the new tokens that are words, not subwords (that do not start with ## or do not are followed by a subword with ##) in the vocabulary of the original BERT tokenizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uY4pga75DlTt"
      },
      "source": [
        "### 1) Let's use a word tokenizer (spaCY) to find the most frequent words of our corpus by using scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoXcomhe103D"
      },
      "source": [
        "**Yes but how?** Let's use a **words tokenizer like spaCY** to find the most frequent words of our corpus instead of a WordPiece tokenizer which generates subwords as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gx6_R5TCWUQP"
      },
      "source": [
        "**Observation**: here, the expression \"most frequent words\" means: the tokens present in most of the documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pEZGsG4xSJj"
      },
      "source": [
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5q6AxoWWxL9c"
      },
      "source": [
        "# initialize our tokenizer with the English spaCY one\n",
        "nlp = spacy.load(\"en_core_web_sm\", exclude=['morphologizer', 'parser', 'ner', 'attribute_ruler', 'lemmatizer'])"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vx0sA1jNRZH1"
      },
      "source": [
        "def spacy_tokenizer(document, nlp=nlp):\n",
        "    # tokenize the document with spaCY\n",
        "    doc = nlp(document)\n",
        "    # Remove stop words and punctuation symbols\n",
        "    tokens = [\n",
        "        token.text for token in doc if (\n",
        "        token.is_stop == False and \\\n",
        "        token.is_punct == False and \\\n",
        "        token.text.strip() != '' and \\\n",
        "        token.text.find(\"\\n\") == -1)]\n",
        "    return tokens\n",
        "\n",
        "def dfreq(idf, N):\n",
        "    return (1+N) / np.exp(idf - 1) - 1"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efQustrf2eCd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c7e4ac1-9e57-45c1-f343-cefaa6f87346"
      },
      "source": [
        "%%time\n",
        "# https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting\n",
        "tfidf_vectorizer = TfidfVectorizer(lowercase=False, tokenizer=spacy_tokenizer, \n",
        "                                   norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
        "# parse matrix of tfidf\n",
        "docs = documents\n",
        "length = len(docs)\n",
        "result = tfidf_vectorizer.fit_transform(docs)\n",
        "# print(result.shape)\n",
        "\n",
        "# idf\n",
        "idf = tfidf_vectorizer.idf_\n",
        "\n",
        "# sorted idf, tokens and docs frequencies\n",
        "idf_sorted_indexes = sorted(range(len(idf)), key=lambda k: idf[k])\n",
        "idf_sorted = idf[idf_sorted_indexes]\n",
        "tokens_by_df = np.array(tfidf_vectorizer.get_feature_names())[idf_sorted_indexes]\n",
        "dfreqs_sorted = dfreq(idf_sorted, length).astype(np.int32)\n",
        "tokens_dfreqs = {tok:dfreq for tok, dfreq in zip(tokens_by_df,dfreqs_sorted)}\n",
        "tokens_pct_list = [int(round(dfreq/length*100,2)) for token,dfreq in tokens_dfreqs.items()]"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4.37 s, sys: 47.9 ms, total: 4.42 s\n",
            "Wall time: 4.42 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7_K5rnqVR3s"
      },
      "source": [
        "# we have only 2 documents (that's why we range the intervale [1,101] with a step of 50)\n",
        "number_tokens_with_DF_above_pct = list()\n",
        "for pct in range(1,101,50):\n",
        "    index_max = len(np.array(tokens_pct_list)[np.array(tokens_pct_list)>=pct])\n",
        "    number_tokens_with_DF_above_pct.append(index_max)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "-MunEm7_Rmnj",
        "outputId": "250b868c-a7c0-4ef1-c76d-5f51d787180d"
      },
      "source": [
        "# DF = Document Frequency\n",
        "\n",
        "# df_docfreqs = pd.DataFrame(number_tokens_with_DF_above_pct, columns=['number of tokens with DF above x%'])\n",
        "# df_docfreqs.index += 1 \n",
        "# df_docfreqs.transpose()\n",
        "\n",
        "# plt.plot(number_tokens_with_DF_above_pct)\n",
        "# plt.title(f'Document Frequency above of {pct}%')\n",
        "# plt.show()\n",
        "\n",
        "df_docfreqs = pd.DataFrame({'pct':list(range(1,101,50)),'number of tokens with DF above pct%':number_tokens_with_DF_above_pct})\n",
        "df_docfreqs.transpose()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>pct</th>\n",
              "      <td>1</td>\n",
              "      <td>51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>number of tokens with DF above pct%</th>\n",
              "      <td>4195</td>\n",
              "      <td>1080</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        0     1\n",
              "pct                                     1    51\n",
              "number of tokens with DF above pct%  4195  1080"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yD7cdavZ4b9"
      },
      "source": [
        "**There are 4186 words which appear in one or two documents from our 2 documents list, and 1058 which are in the 2 documents.**\n",
        "\n",
        "**Let's consider that the 4186 words are all important and relevant to our COVID corpus.**\n",
        "\n",
        "**Observation**: within a corpus with more documents, we could have used another rule as for example: keeping only words which are at least in 10% of the documents list."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oW3KDhoQ7w2R"
      },
      "source": [
        "### Get the vocabulary that is not in the original BERT tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IMxPFT1XK_E"
      },
      "source": [
        "This step is not necessary, as the `tokenizer.add_tokens()` method will add new tokens only if they do not belong to the existing tokenizer vocabulary. However, it helps us to see what these new tokens are."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYJR9G_97w2T"
      },
      "source": [
        "# list of new tokens\n",
        "pct = 1\n",
        "index_max = len(np.array(tokens_pct_list)[np.array(tokens_pct_list)>=pct])\n",
        "new_tokens = tokens_by_df[:index_max]\n",
        "# print(len(new_tokens))\n",
        "\n",
        "old_vocab = [k for k,v in tokenizer.get_vocab().items()]\n",
        "new_vocab = [token for token in new_tokens]\n",
        "idx_old_vocab_list = list()\n",
        "same_tokens_list = list()\n",
        "different_tokens_list = list()\n",
        "\n",
        "for idx_new,w in enumerate(new_vocab): \n",
        "  try:\n",
        "    idx_old = old_vocab.index(w)\n",
        "  except:\n",
        "    idx_old = -1\n",
        "  if idx_old>=0:\n",
        "      idx_old_vocab_list.append(idx_old)\n",
        "      same_tokens_list.append((w,idx_new))\n",
        "  else:\n",
        "      different_tokens_list.append((w,idx_new))"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQwh9j6b7w2U",
        "outputId": "2c296826-ccd9-467e-f9a7-b8df6c37aaf8"
      },
      "source": [
        "len(same_tokens_list),len(different_tokens_list),len(same_tokens_list)+len(different_tokens_list)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3987, 208, 4195)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L69ZMFu47w2W"
      },
      "source": [
        "**We found 226 tokens (whole words) that are not in the vocabulary of the original tokenizer, and the words COVID and hospitalization belong to the new tokens list.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYB3RcY67w2Y",
        "outputId": "e335fb0e-5a77-496c-d90d-0b4861650de9"
      },
      "source": [
        "# get list of new tokens\n",
        "new_tokens = [k for k,v in different_tokens_list]\n",
        "print(len(new_tokens), new_tokens[:20])"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "208 ['0.002', '0.01', '0.1', '0.4', '0.5', '1.4', '1.7', '2.1', '3.4', '4.6', '50,000', '6,174', 'B.1.1.7', 'B.1.351', 'COVID-19', 'CoV-2', 'CoV.', 'P.1', 'U.S.', 'U07.1']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBiR25A-ZksW",
        "outputId": "95fcbff1-1a2b-459f-8032-9407cc8eb797"
      },
      "source": [
        "\"COVID\" in new_tokens, \"hospitalization\" in new_tokens"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(False, True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KILEkSFlB_53"
      },
      "source": [
        "### Add the new tokens (only whole words, not subwords!) in the vocabulary of the original BERT tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yXmxO4sBcsM",
        "outputId": "5c512a8f-793a-4bec-b3ae-28857fd224e3"
      },
      "source": [
        "# import model and tokenizer\n",
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "model_name = \"bert-base-cased\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_name)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJJyqSeRBcsP",
        "outputId": "30794283-312c-4a58-c2c8-60f7c3cb6f0f"
      },
      "source": [
        "print(\"[ BEFORE ] tokenizer vocab size:\", len(tokenizer)) \n",
        "added_tokens = tokenizer.add_tokens(new_tokens)\n",
        "\n",
        "print(\"[ AFTER ] tokenizer vocab size:\", len(tokenizer)) \n",
        "print()\n",
        "print('added_tokens:',added_tokens)\n",
        "print()\n",
        "\n",
        "# resize the embeddings matrix of the model \n",
        "model.resize_token_embeddings(len(tokenizer)) "
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ BEFORE ] tokenizer vocab size: 28996\n",
            "[ AFTER ] tokenizer vocab size: 29204\n",
            "\n",
            "added_tokens: 208\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(29204, 768)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bX4x_NCdBcsR"
      },
      "source": [
        "Let's call tokenizer_exBERT our tokenizer with the new tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeTmSR92BcsS"
      },
      "source": [
        "tokenizer_exBERT = tokenizer"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vmGm43nBcsT",
        "outputId": "5a583914-4a63-4ff4-b6af-d68f7498a84f"
      },
      "source": [
        "# tokenization of the text\n",
        "tokens = tokenizer_exBERT.tokenize(text)\n",
        "print(tokens)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['COVID-19', 'affects', 'different', 'people', 'in', 'different', 'ways', '.', 'Most', 'infected', 'people', 'will', 'develop', 'mild', 'to', 'moderate', 'illness', 'and', 'recover', 'without', 'hospitalization', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HN9Qf_L3BcsU",
        "outputId": "35e47f51-0e2a-4538-87d5-4f01d00f9457"
      },
      "source": [
        "# back to text\n",
        "tokenizer_exBERT.decode(tokenizer_exBERT.encode(text), skip_special_tokens=True)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'COVID-19 affects different people in different ways. Most infected people will develop mild to moderate illness and recover without hospitalization.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4LXqD3rBcsZ"
      },
      "source": [
        "**The tokenizer with the new tokens (only whole words!) did succeed in tokenizing the words COVID and hospitalization correctly (and not only these ones: all of them!)**\n",
        "\n",
        "**It means that is fundamental to add new tokens that are only whole words to an existing subword tokenizer like WordPiece, and not subwords!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYqykZLLBcsb",
        "outputId": "c563ccb0-8cd3-4133-8f81-eb1d50a9f0f3"
      },
      "source": [
        "# tokenization of the words COVID and hospitalization\n",
        "print(tokenizer_exBERT.tokenize('COVID'))\n",
        "print(tokenizer_exBERT.tokenize('hospitalization'))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['COVID']\n",
            "['hospitalization']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hr7NGchV5X6I"
      },
      "source": [
        "## Let's check the impact of our enriched tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PA31nW1s8Fng"
      },
      "source": [
        "Let's use a text about COVID taken from a newspaper site (not from Wikipedia)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMIaazSd5j31"
      },
      "source": [
        "# source: https://edition.cnn.com/2021/04/05/health/us-coronavirus-monday/index.html\n",
        "text = 'Experts say Covid-19 vaccinations in the US are going extremely well -- but not enough people are protected yet and the country may be at the start of another surge. \\\n",
        "The US reported a record over the weekend with more than 4 million Covid-19 vaccine doses administered in 24 hours, according to the Centers for Disease Control and Prevention. \\\n",
        "And the country now averages more than 3 million doses daily, according to CDC data. \\\n",
        "But only about 18.5% of Americans are fully vaccinated, CDC data shows, and Covid-19 cases in the country have recently seen concerning increases. \\\n",
        "\"I do think we still have a few more rough weeks ahead,\" Dr. Celine Gounder, an infectious diseases specialist and epidemiologist, told CNN on Sunday. \\\n",
        "\"What we know from the past year of the pandemic is that we tend to trend about three to four weeks behind Europe in terms of our pandemic patterns.\"'"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xyUZXSn8PhV"
      },
      "source": [
        "Now, let's tokenize this text both with the original BERT tokenizer and its enriched version."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IesgAZnE8haq",
        "outputId": "34d6f5b4-6c5e-4982-d506-6ce1f403eef9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "model_name = \"bert-base-cased\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_name)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSlyHJ-O7NuT",
        "outputId": "7600b492-8fe0-4ed9-ed0b-c8ec91b7ff16"
      },
      "source": [
        "tokens = tokenizer.tokenize(text)\n",
        "print('number of tokens by the original BERT tokenizer:', len(tokens))\n",
        "\n",
        "tokens = tokenizer_exBERT.tokenize(text)\n",
        "print('number of tokens by the enriched tokenizer:', len(tokens))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of tokens by the original BERT tokenizer: 203\n",
            "number of tokens by the enriched tokenizer: 194\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxNQ6ROO8qeN"
      },
      "source": [
        "**As expected, we find that the enriched tokenizer needs less tokens (here, 5%) to tokenize the text on COVID than the original BERT tokenizer.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHXgeeC-l-lJ"
      },
      "source": [
        "## To be continued..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkTSGj-doRdb"
      },
      "source": [
        "Now that we have augmented our tokenizer vocabulary with words specific to our corpus, we need to fine-tune the natural language model it is associated with (here, the bert-base-cased model). Indeed, the addition of new words led to the increase of the matrix of embeddings of the model by the same number: **with each new word added, a new vector of embeddings with random values was added as well** thanks to the `model.resize_token_embeddings(len(tokenizer))` method. So we need to train (or fine-tune) our model on our body so that the model can learn the embeddings of these new words.\n",
        "\n",
        "Hugging Face provided a script and a notebook to fine tune a natural language model on a new corpus (*How to fine-tune a model on language modeling*: [script](https://github.com/huggingface/transformers/tree/master/examples/language-modeling) | [github](https://github.com/huggingface/notebooks/blob/master/examples/language_modeling.ipynb) | [colab](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/language_modeling.ipynb)). **We therefore have a ready-to-use code. However, it is possible that this code is not adapted to your situation** because if the number of new words (and therefore of new embeddings vectors) is high, it is possible that the training by this code leads to a Catastrophic Forgetting by modifying in a sensitive way the vectors of embeddings of the tokens of the initial vocabulary.\n",
        "\n",
        "**My advice**: do a Google search with this type of \"*fine-tune a pre-trained model for a specific domain*\" query. You will get all the interesting articles and documents on this topic. Good job to you!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itMeujJv14E0"
      },
      "source": [
        "# END"
      ]
    }
  ]
}